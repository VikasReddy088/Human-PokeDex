{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Using CapsNet for Video Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load the datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize labels\n",
    "LABELS = set([\"Abuse\", \"Assault\", \"Fighting\", \"Normal\", \"Robbery\", \"Vandalism\"])\n",
    "\n",
    "# Initialize the list of images\n",
    "print(\"Loading images:\")\n",
    "imagePaths = list(paths.list_images(r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Crime Detection\\Datasets'))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "# Loop over the image paths\n",
    "for imagePath in imagePaths:\n",
    "    label = imagePath.split(os.path.sep)[-2]\n",
    "\n",
    "    if label not in LABELS:\n",
    "        continue\n",
    "    \n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = cv2.resize(image, (224, 224))\n",
    "    \n",
    "    data.append(image)\n",
    "    labels.append(label)\n",
    "\n",
    "np.array(labels)\n",
    "np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, stratify=labels, random_state=42)\n",
    "\n",
    "# Initialize the training data augmentation object\n",
    "trainAug = ImageDataGenerator(rotation_range=30, \n",
    "                              zoom_range=0.15, \n",
    "                              width_shift_range=0.2, \n",
    "                              height_shift_range=0.2, \n",
    "                              shear_range=0.15, \n",
    "                              horizontal_flip=True, \n",
    "                              fill_mode=\"nearest\")\n",
    "\n",
    "# Initialize the validation/testing data augmentation object \n",
    "valAug = ImageDataGenerator()\n",
    "\n",
    "# Define the ImageNet mean subtraction (in RGB order) \n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(shape = [None, 224, 224, 3], dtype = tf.float32, name = \"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The first layer will be composed of 256 maps of 104 x 104 capsules each.\n",
    "Each capsule will output a 128 dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps_n_maps = 256\n",
    "caps1_n_caps = caps1_n_maps * 104 * 104                                              # 2768896 capsules\n",
    "caps1_n_dims = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = tf.layers.Conv2D(X, name = \"conv1\", \n",
    "                        filters = 4096, \n",
    "                        kernel_size = 9,\n",
    "                        strides = 1,\n",
    "                        padding = \"valid\",\n",
    "                        activation = tf.nn.relu)\n",
    "\n",
    "conv2 = tf.layers.Conv2D(conv1, name = \"conv2\",\n",
    "                        filters = caps1_n_maps * caps1_n_dims, \n",
    "                        kernel_size = 9, \n",
    "                        strides = 2,\n",
    "                        padding = \"valid\",\n",
    "                        activation = tf.nn.relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the kernel size is 9, the image is shrunk by (9 - 1 = 8) pixels after each Conv2D layer.\n",
    "\n",
    "Hence, after two convolution layers we have (224, 224, 3) -> (216, 216, 3) -> (208, 208, 3).\\\n",
    "Moreover, as stride = 2, (208, 208, 3) -> (104, 104, 3)\n",
    "\n",
    "### Output of the Conv2D layer:\n",
    "\n",
    "Number of maps (256) * Vector dimensions per capsule (128) = 32768 feature maps for each capsule.\\\n",
    "Each feature map is 104 * 104."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caps1_raw = tf.reshape(conv2, [-1, caps1_n_caps, caps1_n_dims], name = \"caps1_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squash(s, axis = -1, epsilon = 1e-7, name = None):\n",
    "    with tf.name_scope(name, default_name=\"squash\"):\n",
    "        squared_norm = tf.reduce_sum(tf.square(s), axis=axis,\n",
    "                                     keep_dims=True)\n",
    "        safe_norm = tf.sqrt(squared_norm + epsilon)\n",
    "        squash_factor = squared_norm / (1. + squared_norm)\n",
    "        unit_vector = s / safe_norm\n",
    "        return squash_factor * unit_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of the first capsule layer\n",
    "caps1_output = squash(caps1_raw, name=\"caps1_output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
