{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wvn_it83\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3df619966c54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mold_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimagePath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m902\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m902\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-wvn_it83\\opencv\\modules\\core\\src\\alloc.cpp:73: error: (-4:Insufficient memory) Failed to allocate 6220800 bytes in function 'cv::OutOfMemoryError'\n"
     ]
    }
   ],
   "source": [
    "LABELS = set([\"Abuse\", \"Assault\", \"Fighting\", \"Normal\", \"Robbery\", \"Vandalism\"])\n",
    "imagePaths = list(paths.list_images(r'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Datasets\\Annotated'))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "clipNos = []\n",
    "\n",
    "init_clipNo = 1\n",
    "clipData = []\n",
    "old_label = \"\"\n",
    "old_clipNo = 0\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    label = imagePath.split(os.path.sep)[-3]\n",
    "    clipNo = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    if (clipNo != init_clipNo):\n",
    "        init_clipNo = clipNo\n",
    "        data.append(clipData)\n",
    "        labels.append(old_label)\n",
    "        clipNos.append(old_clipNo)\n",
    "        clipData = []\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (902, 902))\n",
    "        clipData.append(image)\n",
    "    else:\n",
    "        old_clipNo = clipNo\n",
    "        old_label = label\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (902, 902))\n",
    "        clipData.append(image)\n",
    "    \n",
    "data.append(clipData)\n",
    "labels.append(old_label)\n",
    "clipNos.append(old_clipNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 900, 900, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 900, 900, 64)      256       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 450, 450, 64)      0         \n",
      "=================================================================\n",
      "Total params: 2,048\n",
      "Trainable params: 1,920\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 448, 448, 32)      18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 448, 448, 32)      128       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 224, 224, 32)      0         \n",
      "=================================================================\n",
      "Total params: 18,592\n",
      "Trainable params: 18,528\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the architectures of the first and second layers for temporal convolutions\n",
    "\n",
    "first_layer = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 1, input_shape = (902, 902, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size = (2, 2))\n",
    "])\n",
    "\n",
    "second_layer = tf.keras.models.Sequential([\n",
    "    # input_shape needs to be fixed\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 1, input_shape = first_layer.output.shape[1:]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size = (2, 2))\n",
    "])\n",
    "\n",
    "first_layer.summary()\n",
    "second_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape:  (4, 224, 224, 3)\n",
      "First layer output shape:  (4, 31, 31, 64)\n",
      "Batch_2_1 shape:  (8, 31, 31, 64)\n",
      "Output map dimensions:  (16, 4, 4, 32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to stack",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-06294c6ebaad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m# Figure out how to input 4 images at once to first_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mbatch_1_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mbatch_1_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mbatch_1_3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mc:\\users\\yash umale\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[1;34m(arrays, axis, out)\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 423\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'need at least one array to stack'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    424\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to stack"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "lb = []\n",
    "flag = 0\n",
    "\n",
    "# Loop for each video clip in the entire dataset\n",
    "for i in range(len(data)):\n",
    "    curr = 0\n",
    "    tenFrames = []\n",
    "    label = labels[i]\n",
    "    \n",
    "    # Loop for each 10 frames in a video clip\n",
    "    while ((curr + 10) < len(data[i])):\n",
    "        lb.append(label)\n",
    "        tenFrames = data[i][curr : (curr + 10)]\n",
    "        \n",
    "        batch_1 = tenFrames[curr : curr + 4]\n",
    "        batch_2 = tenFrames[curr + 2 : curr + 6]\n",
    "        batch_3 = tenFrames[curr + 4 : curr + 8]\n",
    "        batch_4 = tenFrames[curr + 6 : curr + 10]\n",
    "        curr += 10\n",
    "        \n",
    "        # Figure out how to input 4 images at once to first_layer\n",
    "        batch_1_1 = np.stack(np.array(batch_1), axis = 0)\n",
    "        batch_1_2 = np.stack(np.array(batch_2), axis = 0)\n",
    "        batch_1_3 = np.stack(np.array(batch_3), axis = 0)\n",
    "        batch_1_4 = np.stack(np.array(batch_4), axis = 0)\n",
    "        \n",
    "        print(\"Input shape: \", batch_1_1.shape)\n",
    "        \n",
    "        # batch_1_1 = batch_1_1.reshape(4, 224, 224, 3)\n",
    "        # batch_1_2 = batch_1_2.reshape(4, 224, 224, 3)\n",
    "        # batch_1_3 = batch_1_3.reshape(4, 224, 224, 3)\n",
    "        # batch_1_4 = batch_1_4.reshape(4, 224, 224, 3)\n",
    "        \n",
    "        out_1_1 = first_layer(batch_1_1)\n",
    "        out_1_2 = first_layer(batch_1_2)\n",
    "        out_1_3 = first_layer(batch_1_3)\n",
    "        out_1_4 = first_layer(batch_1_4)\n",
    "        \n",
    "        print(\"First layer output shape: \", out_1_1.shape)\n",
    "        \n",
    "        batch_2_1 = np.concatenate([out_1_1, out_1_2])\n",
    "        batch_2_2 = np.concatenate([out_1_3, out_1_4])\n",
    "        \n",
    "        print(\"Batch_2_1 shape: \", batch_2_1.shape)\n",
    "        \n",
    "        out_2_1 = second_layer(batch_2_1)\n",
    "        out_2_2 = second_layer(batch_2_2)\n",
    "        output_map = np.concatenate([out_2_1, out_2_2])\n",
    "        \n",
    "        if (flag == 0):\n",
    "            print(\"Output map dimensions: \", output_map.shape)\n",
    "            flag = 1\n",
    "            \n",
    "        output.append(output_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = set([\"Abuse\", \"Assault\", \"Fighting\", \"Normal\", \"Robbery\", \"Vandalism\"])\n",
    "imagePaths = list(paths.list_images(r'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Datasets\\Annotated'))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "clipNos = []\n",
    "\n",
    "init_clipNo = 1\n",
    "clipData = []\n",
    "old_label = \"\"\n",
    "old_clipNo = 0\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    label = imagePath.split(os.path.sep)[-3]\n",
    "    clipNo = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    if (clipNo != init_clipNo):\n",
    "        init_clipNo = clipNo\n",
    "        data.append(clipData)\n",
    "        labels.append(old_label)\n",
    "        clipNos.append(old_clipNo)\n",
    "        clipData = []\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (902, 902))\n",
    "        clipData.append(image)\n",
    "    else:\n",
    "        old_clipNo = clipNo\n",
    "        old_label = label\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (902, 902))\n",
    "        clipData.append(image)\n",
    "    \n",
    "data.append(clipData)\n",
    "labels.append(old_label)\n",
    "clipNos.append(old_clipNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFusedData(clipData, plot):\n",
    "    output = []\n",
    "    for i in range(len(clipData)):\n",
    "        curr = 0\n",
    "        tenFrames = []\n",
    "    \n",
    "        # Loop for each 10 frames in a video clip\n",
    "        while ((curr + 10) <= len(clipData)):\n",
    "            tenFrames = clipData[curr : (curr + 10)]\n",
    "\n",
    "            batch_1 = tenFrames[curr : curr + 4]\n",
    "            batch_2 = tenFrames[curr + 2 : curr + 6]\n",
    "            batch_3 = tenFrames[curr + 4 : curr + 8]\n",
    "            batch_4 = tenFrames[curr + 6 : curr + 10]\n",
    "            curr += 10\n",
    "\n",
    "            # Figure out how to input 4 images at once to first_layer\n",
    "            batch_1_1 = np.stack(np.array(batch_1), axis = 0)\n",
    "            batch_1_2 = np.stack(np.array(batch_2), axis = 0)\n",
    "            batch_1_3 = np.stack(np.array(batch_3), axis = 0)\n",
    "            batch_1_4 = np.stack(np.array(batch_4), axis = 0)\n",
    "\n",
    "            print(\"Input shape: \", batch_1_1.shape)\n",
    "\n",
    "            out_1_1 = first_layer(batch_1_1)\n",
    "            out_1_2 = first_layer(batch_1_2)\n",
    "            out_1_3 = first_layer(batch_1_3)\n",
    "            out_1_4 = first_layer(batch_1_4)\n",
    "\n",
    "            print(\"First layer output shape: \", out_1_1.shape)\n",
    "            \n",
    "            if plot:\n",
    "                square = 8\n",
    "                ix = 1\n",
    "                for _ in range(square):\n",
    "                    for _ in range(square):\n",
    "                        # specify subplot and turn of axis\n",
    "                        ax = plt.subplot(square, square, ix)\n",
    "                        # plot filter channel in grayscale\n",
    "                        # yplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "                        pyplot.imshow(out_1_1[0, :, :, ix-1], cmap='gray')\n",
    "                        ix += 1\n",
    "\n",
    "            batch_2_1 = np.concatenate([out_1_1, out_1_2])\n",
    "            batch_2_2 = np.concatenate([out_1_3, out_1_4])\n",
    "\n",
    "            print(\"Batch_2_1 shape: \", batch_2_1.shape)\n",
    "\n",
    "            out_2_1 = second_layer(batch_2_1)\n",
    "            out_2_2 = second_layer(batch_2_2)\n",
    "            output_map = np.concatenate([out_2_1, out_2_2])\n",
    "\n",
    "            if (flag == 0):\n",
    "                print(\"Output map dimensions: \", output_map.shape)\n",
    "                flag = 1\n",
    "\n",
    "            output.append(output_map)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = set([\"Abuse\", \"Assault\", \"Fighting\", \"Normal\", \"Robbery\", \"Vandalism\"])\n",
    "imgPath = 'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Datasets\\Annotated'\n",
    "imagePaths = list(paths.list_images(r'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Datasets\\Annotated'))\n",
    "\n",
    "labels = os.listdir(imagePaths)\n",
    "lb = []\n",
    "out = []\n",
    "for label in labels:\n",
    "    labelClipPath = imgPath + '\\\\' + label\n",
    "    \n",
    "    clips = os.listdir(labelClipPath)\n",
    "    for clip in clips:\n",
    "        clipData = []\n",
    "        realImagePaths = list(paths.list_images(labelClipPath + '\\\\' + clip))\n",
    "        \n",
    "        for imagePath in realImagePaths:\n",
    "            image = cv2.imread(imagePath)\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, (902, 902))\n",
    "            clipData.append(image)\n",
    "        \n",
    "        output_maps = getFusedOutput(clipData, False)\n",
    "        lb += ([realImagePaths[-2]] * len(output_maps))\n",
    "        out += output_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "# Plot images, their outputs after the first and second layer\n",
    "# One image\n",
    "\n",
    "plt.figure()\n",
    "label = str(input(\"Enter the label: \"))\n",
    "imagePath = 'C:\\\\Users\\\\Yash Umale\\\\Documents\\\\7th Sem\\\\Research Paper\\\\Datasets\\\\Annotated\\\\' + label\n",
    "imagePaths = list(paths.list_images(imagePath))\n",
    "image = pltimg.imread(imagePaths[0])\n",
    "\n",
    "plt.imshow(image)\n",
    "plt.title(label)\n",
    "\n",
    "# Ten images in sequence\n",
    "i = 0\n",
    "for images in imagePaths[:10]:\n",
    "    plt.subplots(2, 5, (i + 1))\n",
    "    image = pltimg.imread(images)\n",
    "    plt.imshow(image)\n",
    "    i += 1\n",
    "    \n",
    "# Output after first layer\n",
    "# Output after second layer\n",
    "_ = getFusedData(imagePaths[:10], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = np.array(out)\n",
    "lbBinarizer = LabelBinarizer()\n",
    "lb = lbBinarizer.fit_transform(lb)\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, lb, test_size = 0.25, stratify = labels, random_state = 42)\n",
    "\n",
    "trainAug = ImageDataGenerator(rotation_range = 30,\n",
    "                             zoom_range = 0.15,\n",
    "                             width_shift_range = 0.2,\n",
    "                             height_shift_range = 0.2,\n",
    "                             shear_range = 0.15,\n",
    "                             horizontal_flip = True,\n",
    "                             fill_mode = \"nearest\")\n",
    "\n",
    "valAug = ImageDataGenerator()\n",
    "\n",
    "mean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\n",
    "trainAug.mean = mean\n",
    "valAug.mean = mean\n",
    "n_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    baseModel = ResNet50(weights = \"imagenet\", include_top = False,\n",
    "        input_tensor = Input(shape = output_map.shape[1:]))\n",
    "\n",
    "    headModel = baseModel.output\n",
    "    headModel = AveragePooling2D(pool_size = (7, 7))(headModel)\n",
    "    headModel = Flatten(name = \"flatten\")(headModel)\n",
    "    headModel = Dense(512, activation = \"relu\")(headModel)\n",
    "    headModel = Dropout(0.5)(headModel)\n",
    "    headModel = Dense(len(lb.classes_), activation = \"softmax\")(headModel)\n",
    "\n",
    "    model = Model(inputs=baseModel.input, outputs=headModel)\n",
    "\n",
    "    for layer in baseModel.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    print(\"Compiling model:\")\n",
    "    opt = SGD(lr = 1e-4, momentum = 0.9, decay = (1e-4 / n_epochs))\n",
    "    model.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n",
    "\n",
    "    print(\"Training head:\")\n",
    "    H = model.fit(\n",
    "        x = trainAug.flow(trainX, trainY, batch_size = 32),\n",
    "        steps_per_epoch = len(trainX) // 32,\n",
    "        validation_data = valAug.flow(testX, testY),\n",
    "        validation_steps = len(testX) // 32,\n",
    "        epochs = n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the network\n",
    "print(\"Evaluating network:\")\n",
    "predictions = model.predict(x = testX.astype(\"float32\"), batch_size = 32)\n",
    "\n",
    "# To print precision, recall, F1 score\n",
    "print(classification_report(testY.argmax(axis = 1), predictions.argmax(axis = 1), target_names = lb.classes_))\n",
    "\n",
    "# Plot the training loss and accuracy\n",
    "N = n_epochs\n",
    "plt.style.use(\"ggplot\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/ Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "\n",
    "plotPath = r'C:\\Users\\Yash Umale\\Documents\\7th Sem\\Research Paper\\Plots'\n",
    "plt.savefig(plotPath)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
