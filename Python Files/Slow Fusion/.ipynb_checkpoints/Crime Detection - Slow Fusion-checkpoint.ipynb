{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "# import the necessary packages\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = set([\"Abuse\", \"Assault\", \"Fighting\", \"Normal\", \"Robbery\", \"Vandalism\"])\n",
    "imagePaths = list(paths.list_images(r'C:\\Users\\Yash Umale\\Documents\\6th Sem\\Open Lab\\Python Files\\Crime Detection\\Datasets'))\n",
    "\n",
    "data = []\n",
    "labels = []\n",
    "clipNos = []\n",
    "\n",
    "init_clipNo = 1\n",
    "clipData = []\n",
    "old_label = \"\"\n",
    "old_clipNo = 0\n",
    "\n",
    "for imagePath in imagePaths:\n",
    "    label = imagePath.split(os.path.sep)[-3]\n",
    "    clipNo = imagePath.split(os.path.sep)[-2]\n",
    "    \n",
    "    if (clipNo != init_clipNo):\n",
    "        init_clipNo = clipNo\n",
    "        data.append(clipData)\n",
    "        labels.append(old_label)\n",
    "        clipNos.append(old_clipNo)\n",
    "        clipData = []\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        clipData.append(image)\n",
    "    else:\n",
    "        old_clipNo = clipNo\n",
    "        old_label = label\n",
    "        \n",
    "        image = cv2.imread(imagePath)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, (224, 224))\n",
    "        clipData.append(image)\n",
    "    \n",
    "data.append(clipData)\n",
    "labels.append(old_label)\n",
    "clipNos.append(old_clipNo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_8 (Conv2D)            (None, 111, 111, 64)      1792      \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 111, 111, 64)      256       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_8 (Average (None, 15, 15, 64)        0         \n",
      "=================================================================\n",
      "Total params: 2,048\n",
      "Trainable params: 1,920\n",
      "Non-trainable params: 128\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 7, 7, 32)          18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 7, 7, 32)          128       \n",
      "_________________________________________________________________\n",
      "average_pooling2d_9 (Average (None, 1, 1, 32)          0         \n",
      "=================================================================\n",
      "Total params: 18,592\n",
      "Trainable params: 18,528\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the architectures of the first and second layers for temporal convolutions\n",
    "\n",
    "first_layer = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(filters = 64, kernel_size = (3, 3), strides = 2, input_shape = (224, 224, 3)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size = (7, 7))\n",
    "])\n",
    "\n",
    "second_layer = tf.keras.models.Sequential([\n",
    "    # input_shape needs to be fixed\n",
    "    tf.keras.layers.Conv2D(filters = 32, kernel_size = (3, 3), strides = 2, input_shape = first_layer.output.shape[1:]),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.AveragePooling2D(pool_size = (7, 7))\n",
    "])\n",
    "\n",
    "first_layer.summary()\n",
    "second_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop for each video clip in the entire dataset\n",
    "for i in range(len(data)):\n",
    "    curr = 0\n",
    "    tenFrames = []\n",
    "    label = labels[i]\n",
    "    \n",
    "    # Loop for each 10 frames in a video clip\n",
    "    while ((curr + 10) < len(data[i])):\n",
    "        tenFrames = data[i][curr : (curr + 10)]\n",
    "        \n",
    "        batch_1 = tenFrames[curr : curr + 4]\n",
    "        batch_2 = tenFrames[curr + 2 : curr + 6]\n",
    "        batch_3 = tenFrames[curr + 4 : curr + 8]\n",
    "        batch_4 = tenFrames[curr + 6 : curr + 10]\n",
    "        curr += 10\n",
    "        \n",
    "        # Figure out how to input 4 images at once to first_layer\n",
    "        batch_1 = np.array(batch_1)\n",
    "        batch_2 = np.array(batch_2)\n",
    "        batch_3 = np.array(batch_3)\n",
    "        batch_4 = np.array(batch_4)\n",
    "        \n",
    "        batch_1 = batch_1.reshape(4, 224, 224, 3)\n",
    "        batch_2 = batch_2.reshape(4, 224, 224, 3)\n",
    "        batch_3 = batch_3.reshape(4, 224, 224, 3)\n",
    "        batch_4 = batch_4.reshape(4, 224, 224, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
